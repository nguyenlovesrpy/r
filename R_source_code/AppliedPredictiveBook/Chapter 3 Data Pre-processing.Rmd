---
title: "Untitled"
author: "Nguyen_LSCM"
date: "7/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(AppliedPredictiveModeling)
library(purrr)
library(MASS)
library(fpp2)
library(caret)


segData <- subset(segmentationOriginal, Case == 'Train')
cellID <- segData$Cell
class <- segData$Class
case <- segData$Case

segData <- segData[, - (1:3)]
statusColNum <- grep("Status", names(segData))

segData <- segData[, -statusColNum]

```
# Transformations 

```{r}
library(e1071)
skewness(segData$AngleCh1)

skewValues <- map(segData, skewness)
skewValues <- apply(segData, 2, skewness)

Ch1AreaTrans <- BoxCoxTrans(segData$AreaCh1)
Ch1AreaTrans

head(segData$AreaCh1)
predict(Ch1AreaTrans, head(segData$AngleCh1))

(819^(-0.9)-1)/(-0.9)

pca0ject <- prcomp(segData, center = TRUE, scale. = TRUE)

percentVariance <- pca0ject$sdev^2/sum(pca0ject$sdev^2)*100
percentVariance[1:3]

head(pca0ject$x[,1:5])

head(pca0ject$rotation[,1:3])

trans <- preProcess(segData, method = c("BoxCox", "center", "scale", "pca"))

transformed <- predict(trans, segData)

head(transformed[, 1:5])

nearZeroVar(segData)

correlations <- cor(segData)

correlations [1:4, 1:4]

library(corrplot)
corrplot(correlations, order ='hclust')

highCorr <- findCorrelation(correlations, cutoff = 0.75)

filteredSegData <- segData [ , -highCorr]

```

# Creating Dummy Variables 

```{r}

data(cars)

carSubset <- cars %>% pivot_longer(cols = c('convertible', 'coupe', 'hatchback', 'sedan', 'wagon'), names_to = "Type") %>% dplyr::select(Type, Price, Mileage)

simpleMod <- dummyVars(~ Mileage + Type, data = carSubset, levelsOnly = TRUE)

# Assumes the effect of the mileage is the same for every type of car


# To have more advanced model, we could assume that there is a joint effect of mileage and car type. This type of effect is referred to as an interaction should be generated. For these data, this adds another 5 predictors to the data frame:



withInteraction <- dummyVars(~ Mileage + Type + Mileage:Type, data = carSubset, levelsOnly = TRUE)

withInteraction

predict(withInteraction, head(carSubset))

```


# Excercise

```{r}

library(mlbench)
data(Glass)

str(Glass)

# Visualization, explore the predictor variables to understand their distributions as well as the relationships between predictors

```

a) Using visualizations, explore the predictor variables to understand their distribution as well as the relationships between predictors

```{r}

suppressMessages(library(tidyverse))
suppressMessages(library(GGally))
suppressMessages(library(psych))

#Descriptive table of predictors in Glass data frame
describe(Glass)

#Visualize the distribution 
ggpairs(data = Glass,Glasslower=list(continuous="points"),upper=list(continuous="blank"),axisLabels="none", switch="both") +theme_bw()

```

b) Do there appear to be any outliers in the data? Are any predictors skewed?


- Yes there are many outliers in dataset. Especially, Type 2 has many outliers in predictors. Meanwhile, Type 7 is the same to type 2, but it only shows this pattern in a few predictors

- All of predictors are skewed. 
+ Left skewed: Mg, Si
+ Right skewed: The rest 

Nearly symmetric distribution: Al, Si

# Data Splitting

```{r}
library(AppliedPredictiveModeling)
data(twoClassData)

str(predictors)

trainingRows <- createDataPartition(classes, p=.8, list = FALSE)
head(trainingRows)

trainPredictors <- predictors[trainingRows, ]
testPredictors <- predictors[-trainingRows, ]
testClasses <- classes[-trainingRows,]

```

# Resampling

The caret package has various functions for data splitting. For example, to use repeated training/test splits, the function ``createDataPartition`` could be used again an addtitional argument named ``times`` to generate multiple splits

```{r}
set.seed(1)

repeatedSplits <- createDataPartition(trainClasses, p =0/.80, times =3)


```

Similarly, the caret package has function ``createResamples`` (for bootstrapping), ``createFolds`` (for k-old cross validation) and ``createMultiFolds`` (for repeated cross0validation). To create indicators for 10-fold cross validation

```{r}
set.seed(1)

cvSplits <- createFolds(trainClasses, k=10, returnTrain = TRUE)

fold1 <- cvSplits[[1]]



```


# Basic Model Building in R

```{r}
modelFunction(price ~ numBedrooms + numBaths + acres, data = housingData)
modelFunction(x = housePredictors, y = price)




```

# Regression

## Computing

The following sections will reference functions from the caret package. To compute model performance, the observed and predicted outcomes should be stored in vectors. For regression, these vectors should be numeric. Here 2 example vectors are manually created t illustrate the techniques (in practice, the vector of predictions would be produced by the model function)


```{r}

observed <- c(0.22, 0.83, -0.12, 0.89, -0.23,-1.3, -0.15, -1.4, 0.62, -.99, -0.18, 0.32, 0.34, -0.3, 0.04, -0.87, 0.55, -1.3,-1.15,0.2, 0.62)

predicted <- c(0.24, 0.78, -0.66, 0.53, 0.7, -0.75, -0.41, -0.43, 0,49, 0.79, -1.19,  0.06, 0.75, -0.07, 0.43, -0.42, -0.25, -0.64, -1.26, -0.07)

residualValues <- observed - predicted





```

